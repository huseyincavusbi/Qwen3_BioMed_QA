{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fwrrpoxi8WP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "bdd623e0-ad12-4f3a-bfad-0e0a159eb7b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.6.2: Fast Qwen3 patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\",\n",
        "    max_seq_length = 2048,   # Context length\n",
        "    load_in_4bit = True, # Load in 4 bit\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e7aecb-3a75-4bf9-9f97-a74e48d60832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.6.2 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # The rank of the LoRA matrices. A higher rank means more trainable parameters, 32 is a balanced choice\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # A list of the specific layers (modules) in the model to apply LoRA to\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # We adapt the most critical parts of the model for our task\n",
        "    lora_alpha = 32,  # The scaling factor for the LoRA updates. A common practice is to set alpha equal to the rank (r).\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # A highly optimized custom implementation of memory-saving technique\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # Rank stabilized LoRA\n",
        "    loftq_config = None,  # LoftQ quantization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SxaOM6c37Ol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "95dc65a4-e1b9-4a8e-de20-0ca1320792a6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"pubmed_qa\", \"pqa_artificial\")\n",
        "\n",
        "# Formatting promt\n",
        "def format_prompt(example):\n",
        "\n",
        "    # The  prompt teaches the model to first state the simple answer, then provide the full explanation from the 'long_answer' field.\n",
        "    prompt = f\"\"\"<|im_start|>system\n",
        "You are a helpful biomedical assistant. Your task is to answer the given question based on the provided context. First, provide a simple 'yes', 'no', or 'maybe' answer, followed by a detailed explanation.<|im_end|>\n",
        "<|im_start|>user\n",
        "Question: {example['question']}\n",
        "Context: {' '.join(example['context']['contexts'])}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "{example['final_decision']}. {example['long_answer']}<|im_end|>\"\"\"\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "# Apply the new formatting function\n",
        "formatted_dataset = dataset.map(\n",
        "    format_prompt,\n",
        "    remove_columns=list(dataset[\"train\"].features),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjgH3lt0e2Sz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9263e3f8-0a62-4c73-f2ed-39df21b3eea6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
              "        num_rows: 211269\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49w5IcPJ4hkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d2cfe10-5433-4060-d44c-04fc770d6517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First example:\n",
            "<|im_start|>system\n",
            "You are a helpful biomedical assistant. Your task is to answer the given question based on the provided context. First, provide a simple 'yes', 'no', or 'maybe' answer, followed by a detailed explanation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Question: Are group 2 innate lymphoid cells ( ILC2s ) increased in chronic rhinosinusitis with nasal polyps or eosinophilia?\n",
            "Context: Chronic rhinosinusitis (CRS) is a heterogeneous disease with an uncertain pathogenesis. Group 2 innate lymphoid cells (ILC2s) represent a recently discovered cell population which has been implicated in driving Th2 inflammation in CRS; however, their relationship with clinical disease characteristics has yet to be investigated. The aim of this study was to identify ILC2s in sinus mucosa in patients with CRS and controls and compare ILC2s across characteristics of disease. A cross-sectional study of patients with CRS undergoing endoscopic sinus surgery was conducted. Sinus mucosal biopsies were obtained during surgery and control tissue from patients undergoing pituitary tumour resection through transphenoidal approach. ILC2s were identified as CD45(+) Lin(-) CD127(+) CD4(-) CD8(-) CRTH2(CD294)(+) CD161(+) cells in single cell suspensions through flow cytometry. ILC2 frequencies, measured as a percentage of CD45(+) cells, were compared across CRS phenotype, endotype, inflammatory CRS subtype and other disease characteristics including blood eosinophils, serum IgE, asthma status and nasal symptom score. 35 patients (40% female, age 48 Â± 17 years) including 13 with eosinophilic CRS (eCRS), 13 with non-eCRS and 9 controls were recruited. ILC2 frequencies were associated with the presence of nasal polyps (P = 0.002) as well as high tissue eosinophilia (P = 0.004) and eosinophil-dominant CRS (P = 0.001) (Mann-Whitney U). They were also associated with increased blood eosinophilia (P = 0.005). There were no significant associations found between ILC2s and serum total IgE and allergic disease. In the CRS with nasal polyps (CRSwNP) population, ILC2s were increased in patients with co-existing asthma (P = 0.03). ILC2s were also correlated with worsening nasal symptom score in CRS (P = 0.04).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "yes. As ILC2s are elevated in patients with CRSwNP, they may drive nasal polyp formation in CRS. ILC2s are also linked with high tissue and blood eosinophilia and have a potential role in the activation and survival of eosinophils during the Th2 immune response. The association of innate lymphoid cells in CRS provides insights into its pathogenesis.<|im_end|>\n",
            "\n",
            "Second example:\n",
            "<|im_start|>system\n",
            "You are a helpful biomedical assistant. Your task is to answer the given question based on the provided context. First, provide a simple 'yes', 'no', or 'maybe' answer, followed by a detailed explanation.<|im_end|>\n",
            "<|im_start|>user\n",
            "Question: Does vagus nerve contribute to the development of steatohepatitis and obesity in phosphatidylethanolamine N-methyltransferase deficient mice?\n",
            "Context: Phosphatidylethanolamine N-methyltransferase (PEMT), a liver enriched enzyme, is responsible for approximately one third of hepatic phosphatidylcholine biosynthesis. When fed a high-fat diet (HFD), Pemt(-/-) mice are protected from HF-induced obesity; however, they develop steatohepatitis. The vagus nerve relays signals between liver and brain that regulate peripheral adiposity and pancreas function. Here we explore a possible role of the hepatic branch of the vagus nerve in the development of diet induced obesity and steatohepatitis in Pemt(-/-) mice. 8-week old Pemt(-/-) and Pemt(+/+) mice were subjected to hepatic vagotomy (HV) or capsaicin treatment, which selectively disrupts afferent nerves, and were compared to sham-operated or vehicle-treatment, respectively. After surgery, mice were fed a HFD for 10 weeks. HV abolished the protection against the HFD-induced obesity and glucose intolerance in Pemt(-/-) mice. HV normalized phospholipid content and prevented steatohepatitis in Pemt(-/-) mice. Moreover, HV increased the hepatic anti-inflammatory cytokine interleukin-10, reduced chemokine monocyte chemotactic protein-1 and the ER stress marker C/EBP homologous protein. Furthermore, HV normalized the expression of mitochondrial electron transport chain proteins and of proteins involved in fatty acid synthesis, acetyl-CoA carboxylase and fatty acid synthase in Pemt(-/-) mice. However, disruption of the hepatic afferent vagus nerve by capsaicin failed to reverse either the protection against the HFD-induced obesity or the development of HF-induced steatohepatitis in Pemt(-/-) mice.<|im_end|>\n",
            "<|im_start|>assistant\n",
            "yes. Neuronal signals via the hepatic vagus nerve contribute to the development of steatohepatitis and protection against obesity in HFD fed Pemt(-/-) mice.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "print(\"First example:\")\n",
        "print(formatted_dataset[\"train\"][0][\"text\"])\n",
        "print(\"\\nSecond example:\")\n",
        "print(formatted_dataset[\"train\"][1][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "b215210a-b5dc-47e2-e437-00fbf72672dd"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = formatted_dataset[\"train\"], # Use the formatted pubmed_qa training split\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run -thousands of steps-\n",
        "        max_steps = 300, # 300 max_steps for demonstration\n",
        "        learning_rate = 2e-5, # Common practice\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ejIt2xSNKKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194b7373-1c09-47ad-d6ea-2f8d63b3bd54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "11.898 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "992840cf-9029-4d7a-fcab-9a3302aa007a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 211,269 | Num Epochs = 1 | Total steps = 300\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 128,450,560/14,000,000,000 (0.92% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCqnaKmlO1U9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acafb609-e4d5-42aa-c90e-55efa5df6a88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7151.5903 seconds used for training.\n",
            "119.19 minutes used for training.\n",
            "Peak reserved memory = 13.963 GB.\n",
            "Peak reserved memory for training = 2.065 GB.\n",
            "Peak reserved memory % of max memory = 94.722 %.\n",
            "Peak reserved memory for training % of max memory = 14.009 %.\n"
          ]
        }
      ],
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# The fine-tuned model and tokenizer are already in memory.\n",
        "# We access the model directly from the completed trainer object.\n",
        "model = trainer.model\n",
        "\n",
        "# Define the conversation using the chat template structure\n",
        "# This is the modern, correct way to format prompts for conversational models.\n",
        "system_prompt = \"You are a helpful biomedical assistant. Your task is to answer the given question based on the provided context. First, provide a simple 'yes', 'no', or 'maybe' answer, followed by a detailed explanation.\"\n",
        "\n",
        "user_question = \"Is there a definitive link between coffee consumption and a reduced risk of Parkinson's disease?\"\n",
        "user_context = \"Several epidemiological studies have suggested an inverse association between coffee consumption and the risk of Parkinson's disease (PD). A large meta-analysis of 26 studies found that the risk of PD was, on average, 30% lower in coffee drinkers compared to non-drinkers. The association appears to be dose-dependent. However, the mechanism is not fully understood, though caffeine's role as an adenosine A2A receptor antagonist is a leading hypothesis. It's important to note that these are observational studies, which show correlation but cannot prove causation. Confounding factors, such as genetics and lifestyle, may also play a role. Randomized controlled trials are needed to establish a causal relationship definitively.\"\n",
        "\n",
        "# Combine the question and context into the user's message\n",
        "user_prompt = f\"Question: {user_question}\\nContext: {user_context}\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "]\n",
        "\n",
        "# Apply the chat template and explicitly disable the \"thinking\" step\n",
        "# This is the key change to get a direct, clean answer. `add_generation_prompt=True` is crucial as it adds the `<|im_start|>assistant\\n`tokens, telling the model where to start its response.\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=False, # This prevents the <think>...</think> block\n",
        ")\n",
        "\n",
        "# Tokenize the prompt and prepare for streaming generation\n",
        "# Move the tokenized inputs to the GPU where the model is.\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# The TextStreamer will print the output token-by-token to the console for a live, typewriter-like effect.\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Generate the response\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"       FINE-TUNED MODEL RESPONSE (Direct Output)\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "_ = model.generate(\n",
        "    **model_inputs,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=256, # Increased for detailed full answers\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        "    do_sample=True, # Recommended for more natural-sounding text\n",
        ")"
      ],
      "metadata": {
        "id": "HYPrha4xOr-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9add92ad-17da-4da6-c7af-9d9977a34aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "       FINE-TUNED MODEL RESPONSE (Direct Output)\n",
            "==================================================\n",
            "\n",
            "Yes, there is a definitive link between coffee consumption and a reduced risk of Parkinson's disease. The evidence from epidemiological studies, including a large meta-analysis of 26 studies, consistently shows a dose-dependent inverse association between coffee consumption and the risk of Parkinson's disease. Caffeine, a major component of coffee, is a leading candidate for the protective effect, as it acts as an adenosine A2A receptor antagonist. However, the exact mechanism remains to be fully elucidated, and further research is needed to establish a causal relationship definitively.<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT0Osy6a_WJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dad9d433-8eb0-45ee-e574-95199bd8872b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model adapters saved successfully to the directory 'qwen3-14b-pubmedqa-lora'\n"
          ]
        }
      ],
      "source": [
        "# The 'trainer' object holds the fine-tuned model.\n",
        "# This command saves the LoRA adapters to a new directory.\n",
        "trainer.save_model(\"qwen3-14b-pubmedqa-lora\")\n",
        "\n",
        "print(\"Model adapters saved successfully to the directory 'qwen3-14b-pubmedqa-lora'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# Define the name for your repository\n",
        "adapters_repo_name = \"huseyincavus/Qwen3-14B-PubMedQA-lora-adapters\"\n",
        "\n",
        "# Set this to True to run the upload.\n",
        "if True:\n",
        "    print(f\"Starting push of LoRA adapters to Hub repository: {adapters_repo_name}\")\n",
        "\n",
        "    # Call the push function with the corrected token.\n",
        "    model.push_to_hub(\n",
        "        adapters_repo_name,\n",
        "        token = hf_token\n",
        "    )\n",
        "\n",
        "    print(f\"\\nSuccessfully pushed LoRA adapters to: https://huggingface.co/{adapters_repo_name}\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping push to Hub. Set the `if` condition to `True` to run.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "YaXqti8J7LCc",
        "outputId": "3b2c568e-9658-46e2-ce87-9f69a7373d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting push of LoRA adapters to Hub repository: huseyincavus/Qwen3-14B-PubMedQA-lora-adapters\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}